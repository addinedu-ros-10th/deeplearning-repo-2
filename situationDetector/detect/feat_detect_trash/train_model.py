import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import os

# --- ê²½ë¡œ ì„¤ì • ---
PROCESSED_TRAIN_DIR = '/home/momo/Desktop/video/processed_data/train'
PROCESSED_TEST_DIR = '/home/momo/Desktop/video/processed_data/test'

# â–¼â–¼â–¼â–¼â–¼ [ë³€ê²½] ëª¨ë“  í´ë¦½ì˜ ê¸¸ì´ë¥¼ í†µì¼í•  í”„ë ˆì„ ìˆ˜ â–¼â–¼â–¼â–¼â–¼
FIXED_FRAMES = 64
# â–²â–²â–²â–²â–² [ë³€ê²½] ì—¬ê¸°ê¹Œì§€ â–²â–²â–²â–²â–²

# --- 1. ë°ì´í„° ë¡œë” ë§Œë“¤ê¸° ---
class VideoDataset(Dataset):
    def __init__(self, data_dir):
        self.data_dir = data_dir
        self.file_list = [f for f in os.listdir(data_dir) if f.endswith('.npy')]

    def __len__(self):
        return len(self.file_list)

    def __getitem__(self, idx):
        file_path = os.path.join(self.data_dir, self.file_list[idx])
        video_data = np.load(file_path)
        label = int(self.file_list[idx].split('_label_')[1].split('.npy')[0])

        # â–¼â–¼â–¼â–¼â–¼ [ë³€ê²½] í”„ë ˆì„ ìˆ˜ë¥¼ ê³ ì • ê¸¸ì´ë¡œ ë§ì¶”ëŠ” ë¡œì§ ì¶”ê°€ â–¼â–¼â–¼â–¼â–¼
        num_frames = video_data.shape[0]
        if num_frames > FIXED_FRAMES:
            # í”„ë ˆì„ ìˆ˜ê°€ ë” ë§ìœ¼ë©´ ì¤‘ê°„ ë¶€ë¶„ì„ ì˜ë¼ ì‚¬ìš©
            start = (num_frames - FIXED_FRAMES) // 2
            video_data = video_data[start : start + FIXED_FRAMES]
        elif num_frames < FIXED_FRAMES:
            # í”„ë ˆì„ ìˆ˜ê°€ ë” ì ìœ¼ë©´ ë§ˆì§€ë§‰ í”„ë ˆì„ì„ ë³µì‚¬í•˜ì—¬ ì±„ì›€
            padding_needed = FIXED_FRAMES - num_frames
            padding = np.tile(video_data[-1:], (padding_needed, 1, 1, 1))
            video_data = np.concatenate([video_data, padding], axis=0)
        # â–²â–²â–²â–²â–² [ë³€ê²½] ì—¬ê¸°ê¹Œì§€ â–²â–²â–²â–²â–²

        video_tensor = torch.tensor(video_data, dtype=torch.float32).permute(3, 0, 1, 2)
        label_tensor = torch.tensor(label, dtype=torch.long)
        
        return video_tensor, label_tensor

train_dataset = VideoDataset(PROCESSED_TRAIN_DIR)
train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)

# --- 2. ëª¨ë¸ ì•„í‚¤í…ì²˜ ì •ì˜ ---
class ActionClassifier(nn.Module):
    def __init__(self):
        super(ActionClassifier, self).__init__()
        self.conv3d = nn.Conv3d(in_channels=3, out_channels=16, kernel_size=(3, 3, 3), padding=1)
        self.relu = nn.ReLU()
        self.pool = nn.MaxPool3d(kernel_size=(2, 2, 2))
        
        # â–¼â–¼â–¼â–¼â–¼ [ë³€ê²½] ê³ ì •ëœ í”„ë ˆì„ ìˆ˜ì— ë§ì¶° ì…ë ¥ í¬ê¸° ì •í™•íˆ ê³„ì‚° â–¼â–¼â–¼â–¼â–¼
        # convì™€ poolì„ ê±°ì¹œ í›„ì˜ ë°ì´í„° í¬ê¸° ê³„ì‚°
        # í”„ë ˆì„: 64 -> 32
        # ë†’ì´/ë„ˆë¹„: 224 -> 112
        final_feature_size = 16 * (FIXED_FRAMES // 2) * (224 // 2) * (224 // 2)
        self.fc1 = nn.Linear(final_feature_size, 2) # 2ëŠ” í´ë˜ìŠ¤ ìˆ˜ (dump, non-dump)
        # â–²â–²â–²â–²â–² [ë³€ê²½] ì—¬ê¸°ê¹Œì§€ â–²â–²â–²â–²â–²

    def forward(self, x):
        x = self.pool(self.relu(self.conv3d(x)))
        x = x.view(x.size(0), -1) 
        x = self.fc1(x)
        return x

# --- 3. í•™ìŠµ ë£¨í”„ ---
model = ActionClassifier()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
num_epochs = 10

print("ğŸš€ ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    
    for videos, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(videos)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}")

print("âœ… ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

# --- 4. ëª¨ë¸ ì €ì¥ ---
MODEL_SAVE_PATH = '/home/momo/Desktop/video/action_model.pt'
torch.save(model.state_dict(), MODEL_SAVE_PATH)
print(f"í•™ìŠµëœ ëª¨ë¸ì´ '{MODEL_SAVE_PATH}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")